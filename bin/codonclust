#!/usr/bin/env python

# CoreTracker Copyright (C) 2016  Emmanuel Noutahi
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.

# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

from sklearn.decomposition import PCA
from sklearn.cluster import DBSCAN, MeanShift, estimate_bandwidth
from sklearn.neighbors import KernelDensity
from json import JSONDecoder
import numpy as np
from sklearn import metrics
import matplotlib.pyplot as plt
import matplotlib.colors as mc
import argparse
import json
from sklearn.preprocessing import StandardScaler
import itertools
from collections import defaultdict
from Bio.Data import CodonTable
import re

codon_list = ["".join(i) for i in itertools.product('ATGC', repeat=3)]
protein_letters = "ACDEFGHIKLMNPQRSTVWY"


def parse_json_file(jfile):
    with open(jfile) as INDATA:
        data = json.load(INDATA)
    return data


def kernel_estimate(dtlen, labels):
    kde = KernelDensity(kernel='gaussian').fit(dtlen)


def print_data_to_file(data, labels, outfile):
    out = outfile.split('.')[0]
    outfile = out + ".txt"
    with open(outfile, 'w') as OUT:
        header = ['Species'] + codon_list
        OUT.write("\t".join(header) + "\n")
        d = data.shape[0]
        for i in xrange(d):
            line = "\t".join([labels[i]] + [str(int(x))
                                            for x in data[i, :]]) + "\n"
            OUT.write(line)


def get_representation(codons, scale=False, outputfile=None, speclist=[]):
    labels = list(codons.keys())
    if speclist:
        labels = list(set(labels) & set(speclist))
    data = np.zeros((len(labels), 64), dtype=np.float)
    for (i, spec) in enumerate(labels):
        data[i, :] = [codons[spec].get(x, 0) for x in codon_list]
    # discard columns where we have zeros
    if outputfile:
        print_data_to_file(data, labels, outputfile)

    cols = np.where(np.sum(data, axis=0) == 0)[0]
    print "The following codon are discarded : ", ",".join([codon_list[x] for x in cols])
    data = np.delete(data, cols, axis=1)
    data_len = np.sum(data, axis=0)
    data = data / data_len
    if scale:
        data = StandardScaler().fit_transform(data)
    return data, labels, data_len


def doPCA(data, n=2):
    pca = PCA(n_components=n)
    return pca.fit_transform(data)


def cluster(data, algo, params={}):

    print_estimate = False
    if algo.upper() == 'DBSCAN':
        algo = DBSCAN(min_samples=4, **params)
    else:
        print_estimate = True
        bandwidth = estimate_bandwidth(data, **params)
        algo = MeanShift(bandwidth=bandwidth, bin_seeding=True)
    labels = algo.fit_predict(data)
    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)
    if print_estimate:
        print('Estimated number of clusters: %d' % n_clusters_)
    #print("Silhouette Coefficient: %0.3f" % metrics.silhouette_score(data, labels))
    return labels


def plot_result(data, labels, output="output.png", algo='MeanShift', params={}):
    clusterlab = cluster(data, algo, params=params)
    unique_labels = set(clusterlab)
    colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))
    fig = plt.figure()
    plt.clf()
    ax = fig.add_subplot(111)
    ax.tick_params(axis='both', which='both', top='off', right='off')
    plt.subplots_adjust(bottom=0.1)
    for k, col in zip(unique_labels, colors):
        if k == -1:
            # Black used for noise.
            col = 'k'
        class_member_mask = (clusterlab == k)
        xy = data[class_member_mask, :2]
        ax.scatter(xy[:, 0], xy[:, 1], marker='o', c=mc.rgb2hex(col), s=500)
    for l, x, y in zip(labels, data[:, 0], data[:, 1]):
        ax.annotate(l, xy=(x, y), xytext=(-12, 15), textcoords='offset points',
                    clip_on=True, multialignment='center', rasterized=True,
                    size='x-small', ha='right', va='center', arrowprops=dict(arrowstyle='->', connectionstyle="arc3"))
    plt.title('PCA and %s Clustering of genome according to codon usage' % algo)
    # plt.show()
    plt.savefig(output, bbox_inches='tight')


def plot_aa_usage(data, codontable, labels, outfile):

    fig = plt.figure()
    plt.clf()
    ax = fig.add_subplot(111)
    colors = plt.cm.Dark2(np.linspace(0, 1, len(labels)))
    x = range(1, len(protein_letters) + 1)
    for (i, spec) in enumerate(labels):
        specdata = defaultdict(int)
        spec_dt_count = 0
        for (cod, l) in data[spec].items():
            if cod != '---':
                specdata[codontable.forward_table[cod].upper()] += l
                spec_dt_count += l
        y = [specdata.get(aa, 0) * 1.0 /
             spec_dt_count for aa in protein_letters]

        plt.plot(x, y, color=colors[i])

    plt.xticks(x, protein_letters)
    plt.subplots_adjust(bottom=0.1)
    plt.margins(0.2)
    plt.xlabel('Amino acid')
    plt.ylabel('frequency')
    plt.title('A.A. frequency in each species')
    plt.legend(labels, loc='center left', bbox_to_anchor=(
        1, 0.5), ncol=3, fancybox=True, shadow=True)
    plt.savefig(outfile, bbox_inches='tight')


if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description='Plot codon or amino acid usage in genome')
    parser.add_argument('--reafile', '-i', dest="reafile",
                        help="Json rea file")
    parser.add_argument('--outfile', '-o', dest="outfile",
                        default="output", help="Outfile file")
    parser.add_argument('--scale', action='store_true',
                        dest="scale", help="Scale data")
    parser.add_argument('--plot_aa_usage', action='store_true',
                        dest="aausage", help="Plot aa usage (frequency)")
    parser.add_argument('--csv', action='store_true', dest="csv",
                        help="Export codon count in csv format")
    parser.add_argument('--speclist', '-s', dest="speclist",
                        help="Only use the provided list of species")
    parser.add_argument('--gcode', dest="defcode", default=4,
                        help="Default genetic code to use")
    parser.add_argument('--pca_component', type=int, dest="pca",
                        default=2, help="Number of component for pca")
    parser.add_argument('--clustering_algo', '--algo', choices=('MeanShift', 'DBSCAN'),
                        default='MeanShift', dest="algo", help="Clustering algorithm to use")
    parser.add_argument('--bandwidth_quantile', dest="ebquant", type=float,
                        default=0.3, help="Quantile value for bandwidth estimation")

    args = parser.parse_args()
    data = parse_json_file(args.reafile)
    speclist = []
    if args.speclist:
        try:
            for line in open(args.speclist):
                line = line.strip()
                if line and not line.startswith('#'):
                    speclist.append(line)
        except IOError:
            speclist = re.split(';|,| |-', args.speclist)
            speclist = [x.strip() for x in speclist]

    dt, labels, _ = get_representation(
        data['codons'], args.scale, (args.outfile if args.csv else None), speclist)
    pca_data = doPCA(dt, args.pca)
    outfile, fmt = args.outfile, 'svg'
    if '.' in outfile:
        outfile, fmt = args.outfile.split('.')

    if args.algo == 'DBSCAN':
        params = {}
    else:
        params = {'quantile': args.ebquant}
    plot_result(pca_data, labels, outfile + "." +
                fmt, algo=args.algo, params=params)

    if args.aausage:
        codontable = CodonTable.unambiguous_dna_by_id[args.defcode]
        outfile = outfile + '_aa_usage'
        plot_aa_usage(data['codons'], codontable, labels, outfile + "." + fmt)

    # TODO: gene length distribution outlier and gene number ==> better in the
    # web code
